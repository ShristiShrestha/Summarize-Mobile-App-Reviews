{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30Qo5xXYuGrS"
   },
   "source": [
    "### Summary Generation for Mobile App Reviews\n",
    "\n",
    "- first, load the sample reviews (N~350)\n",
    "- merge (unique) reviews into a single text\n",
    "- prepare three prompts: chain of density (CoD), chain of density for reviews (CoD_R), and vanilla\n",
    "- use each prompt and generate summaries for each app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFjZ_nkL7upT"
   },
   "outputs": [],
   "source": [
    "import os, re, pandas as pd, time\n",
    "from functools import reduce\n",
    "from openai import OpenAI\n",
    "import json, re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_apps = {\n",
    "  \"ridehailing\": [\"uber\", \"lyft\" ],\n",
    "  \"mentalhealth\": [\"calm\", \"headspace\"],\n",
    "   \"dating\": [\"tinder\", \"bumble\"],\n",
    "   \"investing\": [ \"robinhood\", \"acorn\"]\n",
    "}\n",
    "\n",
    "source_reviews = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain, apps in domains_apps.items():\n",
    "    for app in apps:\n",
    "        df = pd.read_csv(f\"./data/reviews/sampled/{domain}_{app}.csv\")\n",
    "        source_reviews[f\"{domain}_{app}\"] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Generation using CoD, CoD_r, and Vanilla prompting on GPT-4 preview model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ujY46gskTEcl"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens(text, model):\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    assert encoding.decode(encoding.encode(text)) == text\n",
    "    encoder = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoder.encode(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BU9xLmqSCKkJ"
   },
   "outputs": [],
   "source": [
    "task_prompt_cod = \"\"\"Instructions:\n",
    "You will generate increasingly concise, entity-dense summaries of reviews (delimited with XML tags).{{lemma_info}}\n",
    "\n",
    "Repeat the following 2 steps {{iterations}} times.\n",
    "Step 1: Identify {{num_missing_entities}} informative entities (\"\";\"\" delimited) from the reviews which are missing from the previously generated summaries.\n",
    "Step 2: Write a new, denser summary of identical length ({{summary_length}} words) which covers entities from previous summaries plus the Missing entities.\n",
    "\n",
    "A missing entity is:\n",
    "- relevant to the app’s operation,\n",
    "- specific yet concise (5 words or fewer),\n",
    "- novel (not in the previous summary),\n",
    "- faithful (present in the reviews),\n",
    "- anywhere (can be located anywhere in the reviews),\n",
    "- Non-identifier: avoids specific personal, geographical information like name, location, URLs, emails\n",
    "\n",
    "Guidelines:\n",
    "- The first summary should be long (4-5 sentences, {{summary_length}} words) yet highly non-specific, containing little information beyond the entities marked as missing.\n",
    "- Use overly verbose language and fillers (e.g., \"this app discusses\") to reach {{summary_length}} words.\n",
    "- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n",
    "- Make space with fusion, compression, and removal of uninformative phrases like \"the users discuss\".\n",
    "- The summaries should become highly dense and concise yet self-contained, i.e., easily understood without the reviews.\n",
    "- Missing entities can appear anywhere in the reviews. NEVER drop entities from the previous summary.\n",
    "- If space cannot be made, add fewer new entities. Remember, use the exact same number of words for each summary.\n",
    "- Avoid apps' names other than {{app}} in summaries.\n",
    "- Avoid personal and location specific information like name, place, URLs, and emails\n",
    "\n",
    "Remember, use the exact number ({{summary_length}}) of words for each summary.\n",
    "ONLY Answer in JSON. The JSON should be a list (length {{iterations}}) of dictionaries whose keys are \"\"Iteration turn\"\", \"\"Missing_Entities\"\", \"\"Denser_Summary\"\".\n",
    "\"\"\"\n",
    "\n",
    "task_prompt_cod_r = \"\"\"Instructions:\n",
    "You will generate increasingly concise, entity-dense summaries of the reviews of the {{app}} app.\n",
    "Repeat the following 2 steps {{iterations}} times.\n",
    "Step 1:\tIdentify {{num_missing_entities}} informative entities from the reviews which are missing from the previously generated summary.\n",
    "Step 2:\tWrite a new, denser summary of identical length which covers every entity and detail from the previous summary plus the missing entities.\n",
    "Note: An entity is a user-defined issue in the reviews that is perceived to harm or support their goals. This includes the functional or non-functional features of the app.\n",
    "\n",
    "A missing entity is:\n",
    "- relevant to the app’s operation,\n",
    "- specific yet concise (5 words or fewer),\n",
    "- novel (not in the previous summary),\n",
    "- faithful (present in the reviews),\n",
    "- anywhere (can be located anywhere in the reviews).\n",
    "\n",
    "Guidelines:\n",
    "- The first summary should be long (4-5 sentences, {{summary_length}} words) yet highly non-specific, containing little information beyond the entities marked as missing.\n",
    "- Use overly verbose language and fillers (e.g., \"this app discusses\") to reach {{summary_length}} words.\n",
    "- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n",
    "- Make space with fusion, compression, and removal of uninformative phrases like \"the users discuss\".\n",
    "- The summaries should become highly dense and concise yet self-contained, i.e., easily understood without the reviews.\n",
    "- Missing entities can appear anywhere in the reviews. NEVER drop entities from the previous summary.\n",
    "- If space cannot be made, add fewer new entities. Remember, use the exact same number of words for each summary.\n",
    "- Avoid apps' names other than {{app}} in summaries.\n",
    "- Avoid personal and location specific information like name, place, URLs, and emails\n",
    "\n",
    "Remember, use the exact number ({{summary_length}}) of words for each summary.\n",
    "ONLY Answer in JSON. The JSON should be a list (length {{iterations}}) of dictionaries whose keys are \"\"Iteration turn\"\", \"\"Missing_Entities\"\", \"\"Denser_Summary\"\".\n",
    "\"\"\"\n",
    "\n",
    "task_prompt_vanilla = \"\"\"Instructions:\n",
    "You will summarize the app store reviews (delimited with XML tags) for {{app}} app in {{summary_length}} words.{{lemma_info}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_AI_API_KEY = \"your-api-key-goes-here\"\n",
    "client = OpenAI(api_key=OPEN_AI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJ5iIp1yzkQU"
   },
   "outputs": [],
   "source": [
    "def get_reviews(text):\n",
    "    return \"\"\"<REVIEWS>\n",
    "{{reviews}}\n",
    "</REVIEWS>\n",
    "\"\"\"\n",
    "\n",
    "def generate_summary(app, reviews, params, prompt_format, output_file, simply_return=False):\n",
    "    \n",
    "    task_prompt = \"\"\n",
    "    \n",
    "    if prompt_format == \"CoD\"\n",
    "        task_prompt = task_prompt_cod\n",
    "    elif prompt_format == \"CoD_R\":\n",
    "        task_prompt = task_prompt_cod_r\n",
    "    elif prompt_format == \"vanilla\":\n",
    "        task_prompt = task_prompt_vanilla\n",
    "    else:\n",
    "        print(\"Invalid prompt format:\", prompt_format)\n",
    "        return\n",
    "\n",
    "    # get reviews text delimited within <REVIEWS></REVIEWS> tags\n",
    "    reviews_prompt = get_reviews(reviews)\n",
    "    reviews_prompt = reviews_prompt.replace(\"{{reviews}}\", reviews)\n",
    "    reviews_prompt = re.sub('\\n+', '\\n', reviews_prompt)\n",
    "    reviews_prompt = re.sub('\\t+', '\\t', reviews_prompt)\n",
    "\n",
    "    # add params in the task prompt\n",
    "    task_prompt = re.sub(\"{{iterations}}\", params[\"iterations\"], task_prompt)\n",
    "    task_prompt = re.sub(\"{{summary_length}}\", params[\"summary_length\"], task_prompt)\n",
    "    task_prompt = re.sub(\"{{max_summary_length}}\", params[\"max_summary_length\"], task_prompt)\n",
    "    task_prompt = re.sub(\"{{num_missing_entities}}\", params[\"num_missing_entities\"], task_prompt)\n",
    "    task_prompt = re.sub(\"{{app}}\", app, task_prompt)\n",
    "    task_prompt = re.sub(\"\\\"\\\"\", \"\\\"\", task_prompt)\n",
    "\n",
    "    system_prompt = \"You are an experienced summarizer. You will generate summaries that people can understand easily. Reviews will be provided within XML tags (<REVIEWS> and </REVIEWS>).\"\n",
    "\n",
    "    print(system_prompt)\n",
    "    print(reviews_prompt)\n",
    "    print(task_prompt)\n",
    "\n",
    "    # compute tokens for the prompt and reviews texts\n",
    "    reviews_token_len = num_tokens(reviews_prompt, params[\"model\"])\n",
    "    task_token_len = num_tokens(task_prompt, params[\"model\"])\n",
    "    reviews_prompt_len = reviews_token_len + task_token_len\n",
    "    print(\"reviews_token_len: \", reviews_token_len, \n",
    "          \" task_token_len: \", task_token_len, \n",
    "          \" reviews_prompt_len: \", reviews_prompt_len)\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": reviews_prompt},\n",
    "    {\"role\": \"assistant\", \"content\": \"Received reviews delimited with XML tags.\"},\n",
    "    {\"role\": \"user\", \"content\": task_prompt}]\n",
    "    print(\"combined messages:\\n\", messages)\n",
    "\n",
    "    if simply_return:\n",
    "        return\n",
    "\n",
    "    response =  client.chat.completions.create(\n",
    "                model=params[\"model\"],\n",
    "                messages = messages,\n",
    "                n=params[\"num_output\"],\n",
    "                stop= None,\n",
    "                top_p=params[\"top_p\"],\n",
    "                frequency_penalty = params[\"frequency_penalty\"],\n",
    "                presence_penalty = params[\"presence_penalty\"],\n",
    "                temperature=params[\"temperature\"]\n",
    "                )\n",
    "\n",
    "    results = []\n",
    "    for item in response.choices:\n",
    "        content = item.message.content.strip().replace(\"\\n\", \"\")\n",
    "        print(item.index, content)\n",
    "        results.append((item.index, content, task_prompt, reviews_prompt, messages,\n",
    "                        response.model, response.created, response.system_fingerprint,\n",
    "                        response.usage.completion_tokens, response.usage.prompt_tokens, response.usage.total_tokens))\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(results, columns=[\"choice index\", \"content\", \"task prompt\", \"reviews prompt\",\n",
    "                                        \"messages\", \"model\", \"created\", \"system_fingerprint\",\n",
    "                                        \"completion_tokens\", \"prompt_tokens\", \"total_tokens\"])\n",
    "    df.to_csv(output_file, index=False, header=True)\n",
    "    print(\"\\n-------------- saved result to \", output_file, \"------------\\n\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMWcBzTXfIDW"
   },
   "outputs": [],
   "source": [
    "params = {\"iterations\": \"5\", \"summary_length\": \"120\",\n",
    "          \"max_summary_length\": \"120\", \"num_missing_entities\": \"2-4\",\n",
    "          \"prompt_format\": [\"CoD\", \"CoD_R\", \"vanilla\"],\n",
    "    \"top_p\": 0.5, \"temperature\": 0.5, \"max_tokens\": 128000, \"model\": \"gpt-4-1106-preview\",\n",
    "    \"num_output\": 1, \"presence_penalty\": 0.1, \"frequency_penalty\": 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XhYz-C9hDJcy",
    "outputId": "a7d25895-d0e9-4983-b69d-542f734b6363"
   },
   "outputs": [],
   "source": [
    "# generate summaries using each prompt type\n",
    "\n",
    "for domain, apps in domains_apps.items():\n",
    "    for app in apps:\n",
    "        for prompt_format in params[\"prompt_format\"]:\n",
    "            source_df = source_reviews[f\"{domain}_{app}\"]\n",
    "            sample_size = len(source_df.groupby(\"uuid\"))\n",
    "            total_size = len(rate_df)\n",
    "\n",
    "            model = params[\"model\"]\n",
    "            OUTPUT_DIR = f\"./data/summaries/{prompt_format}\"\n",
    "            summ_output_dir = f\"{OUTPUT_DIR}/{model}/{domain}\"\n",
    "            if not os.path.exists(summ_output_dir):\n",
    "                os.makedirs(summ_output_dir)\n",
    "            output_file = f\"{summ_output_dir}/{app}.csv\"\n",
    "            \n",
    "            source_df = source_df.sort_values(\"tfidf score\", ascending=False)\n",
    "            col_reviews = rate_df[\"review\"].tolist()\n",
    "            col_reviews_merged = \".\".join(col_reviews)\n",
    "            \n",
    "            print(\"\\n---------------PARAMS:--------------\\n\", app, params)\n",
    "            print(domain, app, input_format, prompt_format,\n",
    "                    \"\\n#output file: \", output_file, # output file to save summaries\n",
    "                    \"\\n#col reviews len: \",len(col_reviews), # number of \"review\" rows\n",
    "                    \"\\n#reviews: \", len(rate_df.groupby(\"uuid\"))) # number of unique ratings\n",
    "\n",
    "            # call summary generation function\n",
    "            abs_summary(app, col_reviews_merged, params, prompt_format, output_file, True)\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline summary: Hybrid TF.IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_tfidf_summary = {}\n",
    "sents_df = {}\n",
    "\n",
    "for domain, apps in domains_apps.items():\n",
    "    for app in apps:\n",
    "        df = pd.read_csv(f\"./data/raw reviews/{domain}/{app}.csv\")\n",
    "        df = break_into_sentences(df) # defined in data preprocessing file\n",
    "        df = df.apply(lambda row: preprocess(row, \"sent\"), axis=1) # defined in data preprocessing file\n",
    "        sents_df[f\"{domain}_{app}\"] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "from numpy import dot\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    return dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "def select_salient_documents(post_vectors, post_weights, k=10, similarity_threshold=0.4):\n",
    "    \"\"\"\n",
    "        Selects the top k most salient posts in a collection of posts.\n",
    "        To avoid redundancy, any post too similar to other-posts are disregarded. Each selected post will\n",
    "        therefore be both highly salient and representative of unique semantics.\n",
    "\n",
    "        Note:\n",
    "            post_vectors and post_weights must be in the same order. The ith element of post_weights must reflect\n",
    "             the ith element of post_vectors\n",
    "\n",
    "        Args:\n",
    "            post_vectors (list of (list of float)): Hybrid tfidf representation of the documents\n",
    "             as a document-term matrix\n",
    "\n",
    "            post_weights (list of float): Hybrid Tfidf weight for each document\n",
    "\n",
    "            k (int): The number of posts to select as output\n",
    "\n",
    "            similarity_threshold (float): The maximum cosine similiarity for a post to be selected\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sorted_keyed_vectors = [z for _, z in sorted(zip(post_weights, enumerate(post_vectors)), key=lambda i: i[0],\n",
    "                                                 reverse=True)]  # z is (i,vi) sorted by weight\n",
    "\n",
    "    i = 1\n",
    "\n",
    "    veclength = len(post_vectors)\n",
    "    loop_condition = True\n",
    "\n",
    "    significant_indices = [0]\n",
    "    sorted_indices = [sorted_keyed_vectors[0][0]]\n",
    "\n",
    "    while loop_condition:\n",
    "        is_similar = False\n",
    "\n",
    "        for j in significant_indices:\n",
    "            sim = cosine_sim(sorted_keyed_vectors[j][1], sorted_keyed_vectors[i][1])\n",
    "            if sim >= similarity_threshold:\n",
    "                is_similar = True\n",
    "\n",
    "        if not is_similar:\n",
    "            significant_indices.append(i)\n",
    "            sorted_indices.append(sorted_keyed_vectors[i][0])\n",
    "\n",
    "        if (len(significant_indices) >= k) or (i >= veclength - 1):\n",
    "            loop_condition = False\n",
    "        i += 1\n",
    "\n",
    "    return sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pypi.org/project/hybridtfidf/#description\n",
    "\n",
    "from hybridtfidf import HybridTfidf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "        \n",
    "def create_glove_embeddings(reviews, review_sent = False):\n",
    "    reviews_embeddings = []\n",
    "    \n",
    "    for review_str in reviews:\n",
    "        review_words = review_str\n",
    "        if review_sent:\n",
    "            review_words = nltk.word_tokenize(review_str)\n",
    "            \n",
    "        # get embeddings for each word\n",
    "        review_words_embedding = []\n",
    "        for word in review_words:\n",
    "            if word in glove_model.key_to_index: # glove_model is initialized in lib file\n",
    "                review_words_embedding.append(glove_model[word])\n",
    "            else:\n",
    "                review_words_embedding.append(np.zeros(300))\n",
    "        review_embedding = np.mean(review_words_embedding, axis=0)\n",
    "        reviews_embeddings.append(review_embedding)\n",
    "\n",
    "    # Normalize the review embeddings\n",
    "    reviews_embeddings = np.array(reviews_embeddings)\n",
    "    reviews_embeddings /= np.linalg.norm(reviews_embeddings, axis=1).reshape(-1, 1)\n",
    "    return reviews_embeddings\n",
    "    \n",
    "\n",
    "def library_hybrid_tfidf_summary(uuids, raw_reviews, raw_sent, lemma_reviews, output_file):\n",
    "    hybridtfidf = HybridTfidf(threshold=7) \n",
    "    hybridtfidf.fit(lemma_reviews)\n",
    "\n",
    "    # The thresold value affects how strongly the algorithm biases towards longer documents\n",
    "    # A higher threshold will make longer documents have a higher post weight\n",
    "\n",
    "    # list of list (inner list is the hybrid tfidf vector for the document)\n",
    "    # use glove vectors at document level\n",
    "    document_vectors = hybridtfidf.transform(lemma_reviews) \n",
    "    document_weights = hybridtfidf.transform_to_weights(lemma_reviews) # returns a list of hybrid tfidf scores for each document\n",
    "\n",
    "    review_embeddings = create_glove_embeddings(lemma_reviews, True) # each document is a string, so better tokenize\n",
    "    sentence_embeddings = np.array(review_embeddings)\n",
    "    \n",
    "    most_significant = select_salient_documents(sentence_embeddings, document_weights, k = 50, similarity_threshold = 0.5)\n",
    "\n",
    "    dissimilar_reviews = []\n",
    "    for i in most_significant:\n",
    "        dissimilar_reviews.append([i, uuids[i], raw_reviews[i], raw_sent[i], lemma_reviews[i]])       \n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(dissimilar_reviews, columns = [\"index\", \"uuid\", \"review\", \"sent\",  \"lemmatized\"])\n",
    "    df.to_csv(output_file, header=True, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain, apps in domains_apps.items():\n",
    "    for app in apps:\n",
    "        df = sents_df[f\"{domain}_{app}\"]\n",
    "        uuids = df[\"uuid\"].tolist()\n",
    "        raw_reviews = df[\"review\"].tolist()\n",
    "        sent_reviews = df[\"sent\"].tolist()\n",
    "        lemma_sent_reviews = df[\"sent_lemma\"].tolist()\n",
    "        lemmatized_reviews = [re.sub(\",\", \" \", item) for item in lemma_sent_reviews] # array of array\n",
    "        \n",
    "        output_file = f\"./data/summaries/{domain}_{app}_tfidf.csv\"\n",
    "        summary_df = library_hybrid_tfidf_summary(uuids, raw_reviews, sent_reviews, lemmatized_reviews, output_file)\n",
    "        domains_tfidf_summary[domain] = summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Summaries using CoD_r prompting on Gemini 1.5 Flash model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libs and env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os, re, pandas as pd, time, json\n",
    "from functools import reduce\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\")\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GEMINI_API_KEY\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codr_prompt = \"\"\n",
    "\n",
    "with open(\"./data/prompts/codr.txt\", \"r\") as infile:\n",
    "    codr_prompt = infile.read()\n",
    "\n",
    "def get_reviews():\n",
    "    return \"\"\"<REVIEWS>\n",
    "{{reviews}}\n",
    "</REVIEWS>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load reviews for summarization into a single DF \n",
    "- use this DF to generate summaries using llama and gemini models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps_reviews_dir = \"./data/reviews/sampled for summarization\"\n",
    "apps_reviews_df = {}\n",
    "\n",
    "for app_file in [item for item in os.listdir(apps_reviews_dir) if \".csv\" in item]:\n",
    "    app_file_path = f\"{apps_reviews_dir}/{app_file}\"\n",
    "    df = pd.read_csv(app_file_path)\n",
    "    appname = app_file.replace(\".csv\", \"\")\n",
    "    apps_reviews_df[appname] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create API function for summary generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gemini_summary(app, reviews, params, prompt_template, output_file, simply_return=False):\n",
    "    system_prompt = \"You are an experienced summarizer. You will generate summaries that people can understand easily. Reviews will be provided within XML tags (<REVIEWS> and </REVIEWS>).\"\n",
    "\n",
    "    task_prompt = prompt_template\n",
    "\n",
    "    # add params in the task prompt\n",
    "    task_prompt = re.sub(\"{{iterations}}\", params[\"iterations\"], task_prompt)\n",
    "    task_prompt = re.sub(\"{{summary_length}}\", params[\"summary_length\"], task_prompt)\n",
    "    task_prompt = re.sub(\"{{num_missing_entities}}\", params[\"num_missing_entities\"], task_prompt)\n",
    "    task_prompt = re.sub(\"{{app}}\", app, task_prompt)\n",
    "    task_prompt = re.sub(\"\\\"\\\"\", \"\\\"\", task_prompt)\n",
    "    \n",
    "    # init model instance\n",
    "    model_config = genai.GenerationConfig(temperature=params[\"temperature\"],\n",
    "                                      top_p=params[\"top_p\"],\n",
    "                                      max_output_tokens = params[\"max_tokens\"],\n",
    "                                      candidate_count=params[\"num_output\"],\n",
    "                                      presence_penalty=params[\"presence_penalty\"],\n",
    "                                      frequency_penalty=params[\"frequency_penalty\"],\n",
    "                                      response_mime_type = \"application/json\")\n",
    "    print(\"model_config: \", model_config)\n",
    "\n",
    "    model = genai.GenerativeModel(params[\"model\"], \n",
    "                                  system_instruction=system_prompt,\n",
    "                                  generation_config=model_config)\n",
    "    \n",
    "    # count tokens of prompts\n",
    "    system_prompt_tokens = model.count_tokens(system_prompt)\n",
    "    task_prompt_tokens = model.count_tokens(task_prompt)\n",
    "\n",
    "    # WARNING: append \"reviews\" template to the end of the task_prompt\n",
    "    reviews_template  = get_reviews()\n",
    "    task_prompt = f\"{task_prompt}\\n\\n{reviews_template}\" \n",
    "    task_prompt = task_prompt.replace(\"{{reviews}}\", reviews)\n",
    "    task_prompt = re.sub('\\n+', '\\n', task_prompt)\n",
    "    task_prompt = re.sub('\\t+', '\\t', task_prompt)\n",
    "    \n",
    "    print(\"[TASK PROMPT]:\\n\", task_prompt)\n",
    "\n",
    "    # count tokens of app reviews only\n",
    "    reviews_tokens = model.count_tokens(reviews)\n",
    "    \n",
    "    print(\"[TOKENS]:\\t task prompt: \", task_prompt_tokens , \n",
    "          \"\\n\\t reviews: \", reviews_tokens,\n",
    "          \"\\n\\t system prompt: \", system_prompt_tokens)\n",
    "    \n",
    "\n",
    "    if simply_return:\n",
    "        return task_prompt\n",
    "\n",
    "    # call model API\n",
    "    response = model.generate_content(task_prompt)\n",
    "    print(\"[RESPONSE]:\\n\", response)\n",
    "\n",
    "    with open(output_file, \"w\") as outfile:\n",
    "        outfile.write(response.text)\n",
    "        print(\"....saved to file: \", output_file, \"....\")\n",
    "\n",
    "    return task_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Gemini API function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"iterations\": \"5\", \"summary_length\": \"120\", \n",
    "           \"top_p\": 0.5, \"temperature\": 0.5, \n",
    "           \"num_output\": 1, \"max_tokens\": 128000,\n",
    "           \"presence_penalty\": 0.1, \"frequency_penalty\": 0.1,\n",
    "          \"num_missing_entities\": \"2-4\", \"model\": \"gemini-1.5-flash\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All text summaries are checked for JSON format and once verified, text files are converted to .json files\n",
    "\n",
    "_outdir = \"./data/summaries/gemini-summaries\"\n",
    "prompt_text = vanilla_prompt\n",
    "\n",
    "for appname, df in apps_reviews_df.items():\n",
    "    reviews_only = \" \".join(df[\"review\"].tolist())\n",
    "    output_file_prefix = f\"{_outdir}/{appname}\"\n",
    "\n",
    "    output_file = f\"{output_file_prefix}.txt\"\n",
    "    generate_gemini_summary(appname, reviews_only, params, prompt_text, output_file, False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Summaries using CoD_r prompting on Llama 3.1 70B Instruct Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libs and load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.config import Config\n",
    "import json\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_access_key = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "aws_secret_key = os.getenv(\"AWS_SECRET_KEY\")\n",
    "aws_access_key, aws_secret_key\n",
    "\n",
    "model_id = \"us.meta.llama3-1-70b-instruct-v1:0\"\n",
    "config = Config(read_timeout=1000)\n",
    "region_name=\"us-east-1\"\n",
    "\n",
    "aws_client = boto3.client(\"bedrock-runtime\", \n",
    "                        aws_access_key_id=aws_access_key,\n",
    "                        aws_secret_access_key=aws_secret_key,\n",
    "                        region_name=region_name,\n",
    "                        config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Llama API function for summary generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_aws_llama_summary(app, reviews, params, prompt_template, output_file, simply_return=False):\n",
    "    system_prompt = \"You are an experienced summarizer. You will generate summaries that people can understand easily. Reviews will be provided within XML tags (<REVIEWS> and </REVIEWS>).\"\n",
    "\n",
    "    task_prompt = prompt_template\n",
    "\n",
    "    # add params in the task prompt\n",
    "    task_prompt = re.sub(\"{{iterations}}\", params[\"iterations\"], task_prompt)\n",
    "    task_prompt = re.sub(\"{{summary_length}}\", params[\"summary_length\"], task_prompt)\n",
    "    task_prompt = re.sub(\"{{num_missing_entities}}\", params[\"num_missing_entities\"], task_prompt)\n",
    "    task_prompt = re.sub(\"{{app}}\", app, task_prompt)\n",
    "    task_prompt = re.sub(\"\\\"\\\"\", \"\\\"\", task_prompt)\n",
    "    \n",
    "    # append reviews in the task prompt\n",
    "    # WARNING: append \"reviews\" template to the end of the task_prompt\n",
    "    reviews_template = get_reviews()\n",
    "    task_prompt = f\"{system_prompt}\\n{task_prompt}\\n{reviews_template}\" \n",
    "    task_prompt = task_prompt.replace(\"{{reviews}}\", reviews)\n",
    "    task_prompt = re.sub('\\n+', '\\n', task_prompt)\n",
    "    task_prompt = re.sub('\\t+', '\\t', task_prompt)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": task_prompt}],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(\"[TASK PROMPT]:\\n\", task_prompt)\n",
    "    print(\"[MESSAGES]:\\n\", messages)\n",
    "\n",
    "    if simply_return:\n",
    "        return task_prompt\n",
    "\n",
    "\n",
    "    try:\n",
    "        # https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InferenceConfiguration.html\n",
    "        # call model API\n",
    "\n",
    "        model_config = {\"maxTokens\": params[\"max_tokens\"], \n",
    "                             \"temperature\": params[\"temperature\"], \n",
    "                             \"topP\": params[\"top_p\"]}\n",
    "        response = aws_client.converse(\n",
    "            modelId=model_id,\n",
    "            messages=messages,\n",
    "            inferenceConfig=model_config)\n",
    "\n",
    "        # Extract and print the response text.\n",
    "        response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        print(\"[RESPONSE]:\\n\", response)\n",
    "\n",
    "        with open(output_file, \"w\") as outfile:\n",
    "            outfile.write(response_text)\n",
    "            print(\"....saved to file: \", output_file, \"....\")\n",
    "\n",
    "        output_file_2 = output_file.replace(\".txt\", \".json\")\n",
    "        with open(output_file_2, \"w\") as outfile_2:\n",
    "            # outfile_2.write(response)\n",
    "            json.dump(response, outfile_2, indent=4) \n",
    "            print(\"....saved to json file: \", output_file_2, \"....\")\n",
    "\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "        \n",
    "\n",
    "    return task_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the Llama API function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"iterations\": \"5\", \"summary_length\": \"120\", \n",
    "           \"top_p\": 0.5, \"temperature\": 0.5, \n",
    "           \"num_output\": 1, \"max_tokens\": 8192, #128K context length including only 8K output tokens\n",
    "           \"presence_penalty\": 0.1, \"frequency_penalty\": 0.1,\n",
    "          \"num_missing_entities\": \"2-4\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This API offers on-demand service and might take 3-5 minutes to get response from the model.\n",
    "\n",
    "prompt_text = codr_prompt\n",
    "prompt = \"codr\"\n",
    "output_summ_dir = \"./data/summaries/llama-summaries\"\n",
    "\n",
    "for appname, df in apps_reviews_df.items():\n",
    "    reviews_only = \" \".join(df[\"review\"].tolist())\n",
    "    output_file_prefix = f\"{output_summ_dir}/{prompt}/{appname}\"\n",
    "    output_file = f\"{output_file_prefix}.txt\"\n",
    "    generate_aws_llama_summary(appname, reviews_only, params, prompt_text, output_file, False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "30Qo5xXYuGrS",
    "XGxqPgILKhA6"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
